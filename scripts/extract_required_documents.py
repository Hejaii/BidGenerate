#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‹›æ ‡æ–‡ä»¶æ–‡æ¡£æå–å™¨
ä½¿ç”¨é€šä¹‰åƒé—®APIæ™ºèƒ½æå–æ‹›æ ‡æ–‡ä»¶ä¸­è¦æ±‚æäº¤çš„æ‰€æœ‰æ–‡æ¡£ç±»å‹
"""

import json
import os
import sys
from typing import List, Dict, Any
import pdfplumber
from datetime import datetime
import time

from llm_client import LLMClient, DEFAULT_API_KEY

class DocumentExtractor:
    def __init__(self, llm: LLMClient | None = None):
        """åˆå§‹åŒ–æ–‡æ¡£æå–å™¨"""
        self.llm = llm or LLMClient()
        
    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:
        """
        ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹ï¼ŒæŒ‰é¡µåˆ†å‰²
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            åŒ…å«é¡µç å’Œæ–‡æœ¬å†…å®¹çš„å­—å…¸åˆ—è¡¨
        """
        pages_data = []
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                print(f"æ­£åœ¨å¤„ç†PDFæ–‡ä»¶ï¼Œå…± {total_pages} é¡µ...")
                
                for page_num in range(total_pages):
                    page = pdf.pages[page_num]
                    text = page.extract_text()
                    
                    if text and text.strip():
                        pages_data.append({
                            "page": page_num + 1,
                            "text": text.strip(),
                            "content_length": len(text)
                        })
                        
                        if (page_num + 1) % 10 == 0:
                            print(f"å·²å¤„ç† {page_num + 1}/{total_pages} é¡µ")
                            
        except Exception as e:
            print(f"PDFå¤„ç†é”™è¯¯: {e}")
            return []
            
        return pages_data
    
    def analyze_page_content(self, page_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        åˆ†æå•é¡µå†…å®¹ï¼Œè¯†åˆ«æ–‡æ¡£è¦æ±‚
        
        Args:
            page_data: é¡µé¢æ•°æ®å­—å…¸
            
        Returns:
            è¯†åˆ«å‡ºçš„æ–‡æ¡£è¦æ±‚åˆ—è¡¨
        """
        page_num = page_data["page"]
        text = page_data["text"]
        
        # æ„å»ºå‘é€ç»™é€šä¹‰åƒé—®çš„æç¤ºè¯
        prompt = f"""
è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ‹›æ ‡æ–‡ä»¶é¡µé¢å†…å®¹ï¼Œè¯†åˆ«å‡ºæ‰€æœ‰è¦æ±‚æŠ•æ ‡äººæäº¤æˆ–é™„å¸¦çš„æ–‡æ¡£ã€ææ–™ã€è¯æ˜ç­‰ã€‚

é¡µé¢å†…å®¹ï¼š
{text}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œå¦‚æœè¯†åˆ«åˆ°è¦æ±‚æäº¤çš„æ–‡æ¡£ï¼Œè¯·æå–ï¼š
1. æ–‡æ¡£åç§°ï¼ˆå…·ä½“è¦æäº¤ä»€ä¹ˆæ–‡ä»¶ï¼‰
2. åŸå§‹è¦æ±‚è¯­å¥ï¼ˆå®Œæ•´çš„å¥å­ï¼‰
3. æ–‡æ¡£ç±»å‹åˆ†ç±»ï¼ˆèµ„æ ¼æ–‡ä»¶/æŠ€æœ¯æ–‡ä»¶/å•†åŠ¡æ–‡ä»¶/å…¶ä»–ï¼‰

å¦‚æœé¡µé¢ä¸­æ²¡æœ‰ç›¸å…³è¦æ±‚ï¼Œè¯·è¿”å›ç©ºåˆ—è¡¨ã€‚

è¿”å›æ ¼å¼ï¼š
{{
    "documents": [
        {{
            "name": "æ–‡æ¡£åç§°",
            "original_text": "åŸå§‹è¦æ±‚è¯­å¥",
            "category": "æ–‡æ¡£ç±»å‹åˆ†ç±»"
        }}
    ]
}}

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚
"""
        
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹›æ ‡æ–‡ä»¶åˆ†æä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«å’Œæå–æ‹›æ ‡æ–‡ä»¶ä¸­è¦æ±‚æŠ•æ ‡äººæäº¤çš„å„ç§æ–‡æ¡£å’Œææ–™ã€‚",
            },
            {"role": "user", "content": prompt},
        ]

        try:
            api_response = self.llm.chat_json(messages)
        except Exception as e:
            print(f"ç¬¬ {page_num} é¡µAPIè°ƒç”¨å¤±è´¥: {e}")
            return []

        if "documents" in api_response and api_response["documents"]:
            documents = api_response["documents"]
            for doc in documents:
                doc["page"] = page_num
            return documents
        return []
    
    def extract_all_documents(self, pdf_path: str) -> List[Dict[str, Any]]:
        """
        ä»PDFä¸­æå–æ‰€æœ‰è¦æ±‚çš„æ–‡æ¡£
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ‰€æœ‰æ–‡æ¡£è¦æ±‚çš„åˆ—è¡¨
        """
        print("å¼€å§‹æå–PDFæ–‡æœ¬å†…å®¹...")
        pages_data = self.extract_text_from_pdf(pdf_path)
        
        if not pages_data:
            print("PDFæ–‡æœ¬æå–å¤±è´¥")
            return []
        
        print(f"æ–‡æœ¬æå–å®Œæˆï¼Œå…± {len(pages_data)} é¡µ")
        print("å¼€å§‹ä½¿ç”¨é€šä¹‰åƒé—®APIåˆ†ææ¯é¡µå†…å®¹...")
        
        all_documents = []
        
        for i, page_data in enumerate(pages_data):
            print(f"æ­£åœ¨åˆ†æç¬¬ {page_data['page']} é¡µ ({i+1}/{len(pages_data)})...")
            
            documents = self.analyze_page_content(page_data)
            if documents:
                all_documents.extend(documents)
                print(f"ç¬¬ {page_data['page']} é¡µè¯†åˆ«åˆ° {len(documents)} ä¸ªæ–‡æ¡£è¦æ±‚")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™æµ
            time.sleep(1)
        
        return all_documents
    
    def deduplicate_documents(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¯¹æ–‡æ¡£è¦æ±‚è¿›è¡Œå»é‡å¤„ç†
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–‡æ¡£åˆ—è¡¨
        """
        seen = set()
        unique_documents = []
        
        for doc in documents:
            # ä½¿ç”¨æ–‡æ¡£åç§°å’ŒåŸå§‹æ–‡æœ¬çš„ç»„åˆä½œä¸ºå»é‡ä¾æ®
            key = (doc["name"].strip().lower(), doc["original_text"].strip()[:100])
            
            if key not in seen:
                seen.add(key)
                unique_documents.append(doc)
        
        return unique_documents
    
    def categorize_documents(self, documents: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç±»
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æŒ‰ç±»åˆ«åˆ†ç»„çš„æ–‡æ¡£å­—å…¸
        """
        categories = {
            "èµ„æ ¼æ–‡ä»¶": [],
            "æŠ€æœ¯æ–‡ä»¶": [],
            "å•†åŠ¡æ–‡ä»¶": [],
            "å…¶ä»–": []
        }
        
        for doc in documents:
            category = doc.get("category", "å…¶ä»–")
            
            # æ ‡å‡†åŒ–åˆ†ç±»åç§°
            if "èµ„æ ¼" in category or "èµ„è´¨" in category:
                category = "èµ„æ ¼æ–‡ä»¶"
            elif "æŠ€æœ¯" in category:
                category = "æŠ€æœ¯æ–‡ä»¶"
            elif "å•†åŠ¡" in category or "ä»·æ ¼" in category or "æŠ¥ä»·" in category:
                category = "å•†åŠ¡æ–‡ä»¶"
            else:
                category = "å…¶ä»–"
            
            doc["category"] = category
            categories[category].append(doc)
        
        return categories
    
    def generate_markdown(self, documents: List[Dict[str, Any]], categories: Dict[str, List[Dict[str, Any]]]) -> str:
        """
        ç”ŸæˆMarkdownæ ¼å¼çš„æ–‡æ¡£æ¸…å•
        
        Args:
            documents: æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨
            categories: åˆ†ç±»åçš„æ–‡æ¡£
            
        Returns:
            Markdownæ ¼å¼çš„å­—ç¬¦ä¸²
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        markdown_content = f"""# æ‹›æ ‡æ–‡ä»¶è¦æ±‚æäº¤æ–‡æ¡£æ¸…å•

> ç”Ÿæˆæ—¶é—´ï¼š{timestamp}  
> æ–‡æ¡£æ€»æ•°ï¼š{len(documents)} é¡¹

## ğŸ“‹ æ–‡æ¡£æ€»è§ˆ

æœ¬æ¸…å•åŒ…å«æ‹›æ ‡æ–‡ä»¶ä¸­æ˜ç¡®è¦æ±‚æŠ•æ ‡äººæäº¤æˆ–é™„å¸¦çš„æ‰€æœ‰æ–‡æ¡£ã€ææ–™ã€è¯æ˜ç­‰ã€‚

---

## ğŸ“ æŒ‰ç±»åˆ«åˆ†ç±»

"""
        
        # æŒ‰ç±»åˆ«è¾“å‡º
        for category, docs in categories.items():
            if docs:
                markdown_content += f"\n### {category} ({len(docs)} é¡¹)\n\n"
                
                for i, doc in enumerate(docs, 1):
                    markdown_content += f"**{i}. {doc['name']}**\n"
                    markdown_content += f"- é¡µç ï¼šç¬¬ {doc['page']} é¡µ\n"
                    markdown_content += f"- åŸå§‹è¦æ±‚ï¼š{doc['original_text']}\n\n"
        
        # æ·»åŠ å®Œæ•´åˆ—è¡¨
        markdown_content += "\n---\n\n## ğŸ“„ å®Œæ•´æ–‡æ¡£åˆ—è¡¨\n\n"
        
        for i, doc in enumerate(documents, 1):
            markdown_content += f"**{i}. {doc['name']}**\n"
            markdown_content += f"- é¡µç ï¼šç¬¬ {doc['page']} é¡µ\n"
            markdown_content += f"- ç±»åˆ«ï¼š{doc['category']}\n"
            markdown_content += f"- åŸå§‹è¦æ±‚ï¼š{doc['original_text']}\n\n"
        
        return markdown_content
    
    def save_results(self, documents: List[Dict[str, Any]], markdown_content: str, output_dir: str = "."):
        """
        ä¿å­˜æå–ç»“æœ
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            markdown_content: Markdownå†…å®¹
            output_dir: è¾“å‡ºç›®å½•
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜Markdownæ–‡ä»¶
        markdown_file = os.path.join(output_dir, "required_documents.md")
        with open(markdown_file, "w", encoding="utf-8") as f:
            f.write(markdown_content)
        
        # ä¿å­˜JSONæ•°æ®
        json_file = os.path.join(output_dir, f"extracted_documents_{timestamp}.json")
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump({
                "extraction_time": timestamp,
                "total_documents": len(documents),
                "documents": documents
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nç»“æœå·²ä¿å­˜ï¼š")
        print(f"- Markdownæ–‡ä»¶ï¼š{markdown_file}")
        print(f"- JSONæ•°æ®ï¼š{json_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æ‹›æ ‡æ–‡ä»¶æ–‡æ¡£æå–å™¨")
    print("=" * 50)
    
    # æ£€æŸ¥APIå¯†é’¥
    api_key = os.getenv("QIANWEN_API_KEY", DEFAULT_API_KEY)
    llm = LLMClient(api_key=api_key)
    
    # æ£€æŸ¥PDFæ–‡ä»¶
    pdf_path = "03.æ‹›æ ‡æ–‡ä»¶.pdf"
    if not os.path.exists(pdf_path):
        print(f"âŒ é”™è¯¯ï¼šPDFæ–‡ä»¶ä¸å­˜åœ¨ï¼š{pdf_path}")
        sys.exit(1)
    
    # åˆ›å»ºæå–å™¨
    extractor = DocumentExtractor(llm)
    
    try:
        # æå–æ‰€æœ‰æ–‡æ¡£
        print("ğŸš€ å¼€å§‹æå–æ–‡æ¡£è¦æ±‚...")
        documents = extractor.extract_all_documents(pdf_path)
        
        if not documents:
            print("âš ï¸ æœªè¯†åˆ«åˆ°ä»»ä½•æ–‡æ¡£è¦æ±‚")
            return
        
        print(f"âœ… æ–‡æ¡£æå–å®Œæˆï¼Œå…±è¯†åˆ«åˆ° {len(documents)} é¡¹è¦æ±‚")
        
        # å»é‡å’Œåˆ†ç±»
        print("ğŸ”„ æ­£åœ¨è¿›è¡Œå»é‡å’Œåˆ†ç±»...")
        unique_documents = extractor.deduplicate_documents(documents)
        categories = extractor.categorize_documents(unique_documents)
        
        print(f"ğŸ“Š å»é‡åå‰©ä½™ {len(unique_documents)} é¡¹")
        for category, docs in categories.items():
            if docs:
                print(f"  - {category}: {len(docs)} é¡¹")
        
        # ç”ŸæˆMarkdown
        print("ğŸ“ æ­£åœ¨ç”ŸæˆMarkdownæ–‡æ¡£...")
        markdown_content = extractor.generate_markdown(unique_documents, categories)
        
        # ä¿å­˜ç»“æœ
        extractor.save_results(unique_documents, markdown_content)
        
        print("\nğŸ‰ æ–‡æ¡£æå–ä»»åŠ¡å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

